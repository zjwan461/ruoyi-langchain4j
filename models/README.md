---
license: apache-2.0
pipeline_tag: sentence-similarity
tags:
- Sentence Transformers
- sentence-similarity
- sentence-transformers
datasets:
- shibing624/nli_zh
language:
- zh
library_name: sentence-transformers
---


# shibing624/text2vec-base-chinese
This is a CoSENT(Cosine Sentence) model: shibing624/text2vec-base-chinese.

It maps sentences to a 768 dimensional dense vector space and can be used for tasks 
like sentence embeddings, text matching or semantic search.


## Evaluation
For an automated evaluation of this model, see the *Evaluation Benchmark*: [text2vec](https://github.com/shibing624/text2vec)

- chinese text matching taskï¼š

| Arch       | BaseModel                         | Model                                                                                                                                             | ATEC  |  BQ   | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc |    Avg    |  QPS  |
|:-----------|:----------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:|
| Word2Vec   | word2vec                          | [w2v-light-tencent-chinese](https://ai.tencent.com/ailab/nlp/en/download.html)                                                                    | 20.00 | 31.49 | 59.46 | 2.57  | 55.78 |  55.04  |  20.70  |   35.03   | 23769 |
| SBERT      | xlm-roberta-base                  | [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90 |  63.01  |  52.28  |   46.46   | 3138  |
| Instructor | hfl/chinese-roberta-wwm-ext       | [moka-ai/m3e-base](https://huggingface.co/moka-ai/m3e-base)                                                                                       | 41.27 | 63.81 | 74.87 | 12.20 | 76.96 |  75.83  |  60.55  |   57.93   | 2980  |
| CoSENT     | hfl/chinese-macbert-base          | [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)                                                       | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |  70.27  |  50.42  |   51.61   | 3008  |
| CoSENT     | hfl/chinese-lert-large            | [GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese)                                                   | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 |  73.01  |  59.04  |   53.12   | 2092  |
| CoSENT     | nghuyong/ernie-3.0-base-zh        | [shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)                                     | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 |  70.60  |  53.08  |   59.87   | 3089  |
| CoSENT     | nghuyong/ernie-3.0-base-zh        | [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)                                 | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 |  76.70  |  63.30  |    63.08  | 3066  |
| CoSENT     | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2  | [shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)                                             | 32.39 | 50.33 | 65.64 | 32.56 | 74.45 |  68.88  |  51.17  |   53.67   | 4004  |


è¯´æ˜ï¼š
- ç»“æœè¯„æµ‹æŒ‡æ ‡ï¼šspearmanç³»æ•°
- `shibing624/text2vec-base-chinese`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`hfl/chinese-macbert-base`åœ¨ä¸­æ–‡STS-Bæ•°æ®è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­æ–‡STS-Bæµ‹è¯•é›†è¯„ä¼°è¾¾åˆ°è¾ƒå¥½æ•ˆæœï¼Œè¿è¡Œ[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œä¸­æ–‡é€šç”¨è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨
- `shibing624/text2vec-base-chinese-sentence`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`nghuyong/ernie-3.0-base-zh`ç”¨äººå·¥æŒ‘é€‰åçš„ä¸­æ–‡STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)è®­ç»ƒå¾—åˆ°ï¼Œå¹¶åœ¨ä¸­æ–‡å„NLIæµ‹è¯•é›†è¯„ä¼°è¾¾åˆ°è¾ƒå¥½æ•ˆæœï¼Œè¿è¡Œ[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œä¸­æ–‡s2s(å¥å­vså¥å­)è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨
- `shibing624/text2vec-base-chinese-paraphrase`æ¨¡å‹ï¼Œæ˜¯ç”¨CoSENTæ–¹æ³•è®­ç»ƒï¼ŒåŸºäº`nghuyong/ernie-3.0-base-zh`ç”¨äººå·¥æŒ‘é€‰åçš„ä¸­æ–‡STSæ•°æ®é›†[shibing624/nli-zh-all/text2vec-base-chinese-paraphrase-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-paraphrase-dataset)ï¼Œæ•°æ®é›†ç›¸å¯¹äº[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)åŠ å…¥äº†s2p(sentence to paraphrase)æ•°æ®ï¼Œå¼ºåŒ–äº†å…¶é•¿æ–‡æœ¬çš„è¡¨å¾èƒ½åŠ›ï¼Œå¹¶åœ¨ä¸­æ–‡å„NLIæµ‹è¯•é›†è¯„ä¼°è¾¾åˆ°SOTAï¼Œè¿è¡Œ[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)ä»£ç å¯è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹æ–‡ä»¶å·²ç»ä¸Šä¼ HF model hubï¼Œä¸­æ–‡s2p(å¥å­vsæ®µè½)è¯­ä¹‰åŒ¹é…ä»»åŠ¡æ¨èä½¿ç”¨
- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`æ¨¡å‹æ˜¯ç”¨SBERTè®­ç»ƒï¼Œæ˜¯`paraphrase-MiniLM-L12-v2`æ¨¡å‹çš„å¤šè¯­è¨€ç‰ˆæœ¬ï¼Œæ”¯æŒä¸­æ–‡ã€è‹±æ–‡ç­‰
- `w2v-light-tencent-chinese`æ˜¯è…¾è®¯è¯å‘é‡çš„Word2Vecæ¨¡å‹ï¼ŒCPUåŠ è½½ä½¿ç”¨ï¼Œé€‚ç”¨äºä¸­æ–‡å­—é¢åŒ¹é…ä»»åŠ¡å’Œç¼ºå°‘æ•°æ®çš„å†·å¯åŠ¨æƒ…å†µ

## Usage (text2vec)
Using this model becomes easy when you have [text2vec](https://github.com/shibing624/text2vec) installed:

```
pip install -U text2vec
```

Then you can use the model like this:

```python
from text2vec import SentenceModel
sentences = ['å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡', 'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡']

model = SentenceModel('shibing624/text2vec-base-chinese')
embeddings = model.encode(sentences)
print(embeddings)
```

## Usage (HuggingFace Transformers)
Without [text2vec](https://github.com/shibing624/text2vec), you can use the model like this: 

First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.

Install transformers:
```
pip install transformers
```

Then load model and predict:
```python
from transformers import BertTokenizer, BertModel
import torch

# Mean Pooling - Take attention mask into account for correct averaging
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

# Load model from HuggingFace Hub
tokenizer = BertTokenizer.from_pretrained('shibing624/text2vec-base-chinese')
model = BertModel.from_pretrained('shibing624/text2vec-base-chinese')
sentences = ['å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡', 'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡']
# Tokenize sentences
encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')

# Compute token embeddings
with torch.no_grad():
    model_output = model(**encoded_input)
# Perform pooling. In this case, mean pooling.
sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])
print("Sentence embeddings:")
print(sentence_embeddings)
```

## Usage (sentence-transformers)
[sentence-transformers](https://github.com/UKPLab/sentence-transformers) is a popular library to compute dense vector representations for sentences.

Install sentence-transformers:
```
pip install -U sentence-transformers
```

Then load model and predict:

```python
from sentence_transformers import SentenceTransformer

m = SentenceTransformer("shibing624/text2vec-base-chinese")
sentences = ['å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡', 'èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡']

sentence_embeddings = m.encode(sentences)
print("Sentence embeddings:")
print(sentence_embeddings)
```

## Model speed up


| Model                                                                                                                        | ATEC              | BQ                | LCQMC            | PAWSX            | STSB             |
|------------------------------------------------------------------------------------------------------------------------------|-------------------|-------------------|------------------|------------------|------------------|
| shibing624/text2vec-base-chinese (fp32, baseline)                                                                            | 0.31928           | 0.42672           | 0.70157          | 0.17214          | 0.79296          |
| shibing624/text2vec-base-chinese (onnx-O4, [#29](https://huggingface.co/shibing624/text2vec-base-chinese/discussions/29))    | 0.31928           | 0.42672           | 0.70157          | 0.17214          | 0.79296          |
| shibing624/text2vec-base-chinese (ov, [#27](https://huggingface.co/shibing624/text2vec-base-chinese/discussions/27))         | 0.31928           | 0.42672           | 0.70157          | 0.17214          | 0.79296          |
| shibing624/text2vec-base-chinese (ov-qint8, [#30](https://huggingface.co/shibing624/text2vec-base-chinese/discussions/30))   | 0.30778 (-3.60%)  | 0.43474 (+1.88%)  | 0.69620 (-0.77%) | 0.16662 (-3.20%) | 0.79396 (+0.13%) |

In short:
1. âœ… shibing624/text2vec-base-chinese (onnx-O4), ONNX Optimized to [O4](https://huggingface.co/docs/optimum/en/onnxruntime/usage_guides/optimization) does not reduce performance, but gives a [~2x speedup](https://sbert.net/docs/sentence_transformer/usage/efficiency.html#benchmarks) on GPU.
2. âœ… shibing624/text2vec-base-chinese (ov), OpenVINO does not reduce performance, but gives a 1.12x speedup on CPU.
3. ğŸŸ¡ shibing624/text2vec-base-chinese (ov-qint8), int8 quantization with OV incurs a small performance hit on some tasks, and a tiny performance gain on others, when quantizing with [Chinese STSB](https://huggingface.co/datasets/PhilipMay/stsb_multi_mt). Additionally, it results in a [4.78x speedup](https://sbert.net/docs/sentence_transformer/usage/efficiency.html#benchmarks) on CPU.

- usage: shibing624/text2vec-base-chinese (onnx-O4), for gpu
```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer(
    "shibing624/text2vec-base-chinese",
    backend="onnx",
    model_kwargs={"file_name": "model_O4.onnx"},
)
embeddings = model.encode(["å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡", "èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡", "ä½ æ˜¯è°"])
print(embeddings.shape)
similarities = model.similarity(embeddings, embeddings)
print(similarities)
```


- usage: shibing624/text2vec-base-chinese (ov), for cpu
```python
# pip install 'optimum[openvino]'

from sentence_transformers import SentenceTransformer

model = SentenceTransformer(
    "shibing624/text2vec-base-chinese",
    backend="openvino",
)

embeddings = model.encode(["å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡", "èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡", "ä½ æ˜¯è°"])
print(embeddings.shape)
similarities = model.similarity(embeddings, embeddings)
print(similarities)
```

- usage: shibing624/text2vec-base-chinese (ov-qint8), for cpu
```python
# pip install optimum
from sentence_transformers import SentenceTransformer

model = SentenceTransformer(
    "shibing624/text2vec-base-chinese",
    backend="onnx",
    model_kwargs={"file_name": "model_qint8_avx512_vnni.onnx"},
)
embeddings = model.encode(["å¦‚ä½•æ›´æ¢èŠ±å‘—ç»‘å®šé“¶è¡Œå¡", "èŠ±å‘—æ›´æ”¹ç»‘å®šé“¶è¡Œå¡", "ä½ æ˜¯è°"])
print(embeddings.shape)
similarities = model.similarity(embeddings, embeddings)
print(similarities)
```


## Full Model Architecture
```
CoSENT(
  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel 
  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_mean_tokens': True})
)
```

## Intended uses

Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures 
the semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.

By default, input text longer than 256 word pieces is truncated.


## Training procedure

### Pre-training 

We use the pretrained [`hfl/chinese-macbert-base`](https://huggingface.co/hfl/chinese-macbert-base) model. 
Please refer to the model card for more detailed information about the pre-training procedure.

### Fine-tuning 

We fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each 
possible sentence pairs from the batch.
We then apply the rank loss by comparing with true pairs and false pairs.

#### Hyper parameters

- training dataset: https://huggingface.co/datasets/shibing624/nli_zh
- max_seq_length: 128
- best epoch: 5
- sentence embedding dim: 768



## Citing & Authors
This model was trained by [text2vec](https://github.com/shibing624/text2vec). 
        
If you find this model helpful, feel free to cite:
```bibtex 
@software{text2vec,
  author = {Xu Ming},
  title = {text2vec: A Tool for Text to Vector},
  year = {2022},
  url = {https://github.com/shibing624/text2vec},
}
```